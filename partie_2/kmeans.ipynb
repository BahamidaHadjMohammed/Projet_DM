{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b34e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans as SklearnKMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, davies_bouldin_score, calinski_harabasz_score,\n",
    "    adjusted_rand_score, normalized_mutual_info_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KMeansFromScratch:\n",
    "    \"\"\"\n",
    "    K-Means Clustering implemented from scratch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters=3, max_iters=300, tolerance=1e-4, random_state=42):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_clusters : int\n",
    "            Number of clusters\n",
    "        max_iters : int\n",
    "            Maximum number of iterations\n",
    "        tolerance : float\n",
    "            Convergence threshold\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        # self.tolerance = tolerance\n",
    "        self.random_state = random_state\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "        self.inertia_ = None\n",
    "        self.n_iter_ = 0\n",
    "\n",
    "    def _initialize_centroids(self, X):\n",
    "        \"\"\"Initialize centroids using random selection\"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        random_indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
    "        centroids = X[random_indices]\n",
    "        return centroids\n",
    "\n",
    "    def _calculate_distance(self, X, centroids):\n",
    "        \"\"\"Calculate Euclidean distance between points and centroids\"\"\"\n",
    "        distances = np.zeros((X.shape[0], self.n_clusters))\n",
    "\n",
    "        for k in range(self.n_clusters):\n",
    "            # Euclidean distance: sqrt(sum((x - centroid)^2))\n",
    "            distances[:, k] = np.sqrt(np.sum((X - centroids[k])**2, axis=1))\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def _assign_clusters(self, distances):\n",
    "        \"\"\"Assign each point to nearest centroid\"\"\"\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def _update_centroids(self, X, labels):\n",
    "        \"\"\"Update centroids as mean of assigned points\"\"\"\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "\n",
    "        for k in range(self.n_clusters):\n",
    "            # Get all points assigned to cluster k\n",
    "            cluster_points = X[labels == k]\n",
    "\n",
    "            if len(cluster_points) > 0:\n",
    "                # Calculate mean of cluster points\n",
    "                centroids[k] = np.mean(cluster_points, axis=0)\n",
    "            else:\n",
    "                # If cluster is empty, reinitialize randomly\n",
    "                centroids[k] = X[np.random.choice(X.shape[0])]\n",
    "\n",
    "        return centroids\n",
    "\n",
    "    def _calculate_inertia(self, X, labels, centroids):\n",
    "        \"\"\"Calculate within-cluster sum of squares (inertia)\"\"\"\n",
    "        inertia = 0\n",
    "        for k in range(self.n_clusters):\n",
    "            cluster_points = X[labels == k]\n",
    "            if len(cluster_points) > 0:\n",
    "                inertia += np.sum((cluster_points - centroids[k])**2)\n",
    "        return inertia\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit K-Means clustering\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        \"\"\"\n",
    "        # Convert to numpy array\n",
    "        X = np.array(X)\n",
    "\n",
    "        # Initialize centroids\n",
    "        self.centroids = self._initialize_centroids(X)\n",
    "\n",
    "        # Iterative optimization\n",
    "        for iteration in range(self.max_iters):\n",
    "            # Store old centroids for convergence check\n",
    "            old_centroids = self.centroids.copy()\n",
    "\n",
    "            # Step 1: Calculate distances to all centroids\n",
    "            distances = self._calculate_distance(X, self.centroids)\n",
    "\n",
    "            # Step 2: Assign points to nearest centroid\n",
    "            self.labels_ = self._assign_clusters(distances)\n",
    "\n",
    "            # Step 3: Update centroids\n",
    "            self.centroids = self._update_centroids(X, self.labels_)\n",
    "\n",
    "            # Check for convergence\n",
    "            centroid_shift = np.sqrt(np.sum((self.centroids - old_centroids)**2))\n",
    "\n",
    "            if centroid_shift < self.tolerance:\n",
    "                self.n_iter_ = iteration + 1\n",
    "                break\n",
    "        else:\n",
    "            self.n_iter_ = self.max_iters\n",
    "\n",
    "        # Calculate final inertia\n",
    "        self.inertia_ = self._calculate_inertia(X, self.labels_, self.centroids)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels for new data\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            New data\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        labels : array, shape (n_samples,)\n",
    "            Cluster labels\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        distances = self._calculate_distance(X, self.centroids)\n",
    "        return self._assign_clusters(distances)\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Fit and predict in one step\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "\n",
    "\n",
    "class KMeansComparison:\n",
    "    \"\"\"\n",
    "    Compare custom K-Means with sklearn implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, true_labels=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Data to cluster\n",
    "        true_labels : array-like, optional\n",
    "            True labels if available (for evaluation)\n",
    "        \"\"\"\n",
    "        self.X = np.array(X)\n",
    "        self.true_labels = true_labels\n",
    "        self.results = {}\n",
    "\n",
    "    def run_comparison(self, k_values=[2, 3, 4, 5, 6], random_state=42):\n",
    "        \"\"\"\n",
    "        Run both implementations for different k values\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        k_values : list\n",
    "            List of k values to test\n",
    "        random_state : int\n",
    "            Random seed\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"K-MEANS COMPARISON: FROM SCRATCH VS SKLEARN\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for k in k_values:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Testing with K = {k}\")\n",
    "            print('='*70)\n",
    "\n",
    "            # Custom Implementation\n",
    "            print(\"\\n[1] Custom K-Means (From Scratch)\")\n",
    "            print(\"-\" * 50)\n",
    "            start_time = time.time()\n",
    "\n",
    "            custom_kmeans = KMeansFromScratch(\n",
    "                n_clusters=k,\n",
    "                max_iters=300,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            custom_labels = custom_kmeans.fit_predict(self.X)\n",
    "\n",
    "            custom_time = time.time() - start_time\n",
    "            custom_inertia = custom_kmeans.inertia_\n",
    "            custom_iterations = custom_kmeans.n_iter_\n",
    "\n",
    "            print(f\"✓ Time taken: {custom_time:.4f} seconds\")\n",
    "            print(f\"✓ Iterations: {custom_iterations}\")\n",
    "            print(f\"✓ Inertia: {custom_inertia:.2f}\")\n",
    "\n",
    "            # Sklearn Implementation\n",
    "            print(\"\\n[2] Sklearn K-Means\")\n",
    "            print(\"-\" * 50)\n",
    "            start_time = time.time()\n",
    "\n",
    "            sklearn_kmeans = SklearnKMeans(\n",
    "                n_clusters=k,\n",
    "                max_iter=300,\n",
    "                random_state=random_state,\n",
    "                n_init=10  # sklearn runs 10 times by default\n",
    "            )\n",
    "            sklearn_labels = sklearn_kmeans.fit_predict(self.X)\n",
    "\n",
    "            sklearn_time = time.time() - start_time\n",
    "            sklearn_inertia = sklearn_kmeans.inertia_\n",
    "            sklearn_iterations = sklearn_kmeans.n_iter_\n",
    "\n",
    "            print(f\"✓ Time taken: {sklearn_time:.4f} seconds\")\n",
    "            print(f\"✓ Iterations: {sklearn_iterations}\")\n",
    "            print(f\"✓ Inertia: {sklearn_inertia:.2f}\")\n",
    "\n",
    "            # Evaluation Metrics\n",
    "            print(\"\\n[3] Clustering Quality Metrics\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            \"\"\"\n",
    "            | Metric            | Measures                               | Best Value | Sensitive To            |\n",
    "            | ----------------- | -------------------------------------- | ---------- | ----------------------- |\n",
    "            | Silhouette        | Compactness + separation (point-level) | Higher     | Overlapping clusters    |\n",
    "            | Davies–Bouldin    | Worst-case cluster similarity          | Lower      | Noisy / spread clusters |\n",
    "            | Calinski–Harabasz | Global variance ratio                  | Higher     | Number of clusters      |\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # Silhouette Score (higher is better, range: -1 to 1)\n",
    "            custom_silhouette = silhouette_score(self.X, custom_labels)\n",
    "            sklearn_silhouette = silhouette_score(self.X, sklearn_labels)\n",
    "\n",
    "            # Davies-Bouldin Index (lower is better)\n",
    "            custom_db = davies_bouldin_score(self.X, custom_labels)\n",
    "            sklearn_db = davies_bouldin_score(self.X, sklearn_labels)\n",
    "\n",
    "            # Calinski-Harabasz Index (higher is better)\n",
    "            custom_ch = calinski_harabasz_score(self.X, custom_labels)\n",
    "            sklearn_ch = calinski_harabasz_score(self.X, sklearn_labels)\n",
    "\n",
    "            print(f\"Silhouette Score:\")\n",
    "            print(f\"  Custom:  {custom_silhouette:.4f}\")\n",
    "            print(f\"  Sklearn: {sklearn_silhouette:.4f}\")\n",
    "            print(f\"\\nDavies-Bouldin Index (lower is better):\")\n",
    "            print(f\"  Custom:  {custom_db:.4f}\")\n",
    "            print(f\"  Sklearn: {sklearn_db:.4f}\")\n",
    "            print(f\"\\nCalinski-Harabasz Index (higher is better):\")\n",
    "            print(f\"  Custom:  {custom_ch:.2f}\")\n",
    "            print(f\"  Sklearn: {sklearn_ch:.2f}\")\n",
    "\n",
    "            # If true labels available, calculate external validation\n",
    "            if self.true_labels is not None:\n",
    "                custom_ari = adjusted_rand_score(self.true_labels, custom_labels)\n",
    "                sklearn_ari = adjusted_rand_score(self.true_labels, sklearn_labels)\n",
    "\n",
    "                custom_nmi = normalized_mutual_info_score(self.true_labels, custom_labels)\n",
    "                sklearn_nmi = normalized_mutual_info_score(self.true_labels, sklearn_labels)\n",
    "\n",
    "                print(f\"\\nAdjusted Rand Index (vs true labels):\")\n",
    "                print(f\"  Custom:  {custom_ari:.4f}\")\n",
    "                print(f\"  Sklearn: {sklearn_ari:.4f}\")\n",
    "                print(f\"\\nNormalized Mutual Info (vs true labels):\")\n",
    "                print(f\"  Custom:  {custom_nmi:.4f}\")\n",
    "                print(f\"  Sklearn: {sklearn_nmi:.4f}\")\n",
    "            else:\n",
    "                custom_ari = sklearn_ari = None\n",
    "                custom_nmi = sklearn_nmi = None\n",
    "\n",
    "            # Store results\n",
    "            self.results[k] = {\n",
    "                'custom': {\n",
    "                    'model': custom_kmeans,\n",
    "                    'labels': custom_labels,\n",
    "                    'time': custom_time,\n",
    "                    'inertia': custom_inertia,\n",
    "                    'iterations': custom_iterations,\n",
    "                    'silhouette': custom_silhouette,\n",
    "                    'davies_bouldin': custom_db,\n",
    "                    'calinski_harabasz': custom_ch,\n",
    "                    'ari': custom_ari,\n",
    "                    'nmi': custom_nmi\n",
    "                },\n",
    "                'sklearn': {\n",
    "                    'model': sklearn_kmeans,\n",
    "                    'labels': sklearn_labels,\n",
    "                    'time': sklearn_time,\n",
    "                    'inertia': sklearn_inertia,\n",
    "                    'iterations': sklearn_iterations,\n",
    "                    'silhouette': sklearn_silhouette,\n",
    "                    'davies_bouldin': sklearn_db,\n",
    "                    'calinski_harabasz': sklearn_ch,\n",
    "                    'ari': sklearn_ari,\n",
    "                    'nmi': sklearn_nmi\n",
    "                }\n",
    "            }\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"✓ COMPARISON COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    def plot_comparison(self):\n",
    "        \"\"\"Generate comprehensive comparison visualizations\"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"GENERATING COMPARISON VISUALIZATIONS\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        k_values = list(self.results.keys())\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        custom_metrics = {\n",
    "            'inertia': [],\n",
    "            'silhouette': [],\n",
    "            'davies_bouldin': [],\n",
    "            'time': [],\n",
    "            'iterations': []\n",
    "        }\n",
    "\n",
    "        sklearn_metrics = {\n",
    "            'inertia': [],\n",
    "            'silhouette': [],\n",
    "            'davies_bouldin': [],\n",
    "            'time': [],\n",
    "            'iterations': []\n",
    "        }\n",
    "\n",
    "        for k in k_values:\n",
    "            custom_metrics['inertia'].append(self.results[k]['custom']['inertia'])\n",
    "            custom_metrics['silhouette'].append(self.results[k]['custom']['silhouette'])\n",
    "            custom_metrics['davies_bouldin'].append(self.results[k]['custom']['davies_bouldin'])\n",
    "            custom_metrics['time'].append(self.results[k]['custom']['time'])\n",
    "            custom_metrics['iterations'].append(self.results[k]['custom']['iterations'])\n",
    "\n",
    "            sklearn_metrics['inertia'].append(self.results[k]['sklearn']['inertia'])\n",
    "            sklearn_metrics['silhouette'].append(self.results[k]['sklearn']['silhouette'])\n",
    "            sklearn_metrics['davies_bouldin'].append(self.results[k]['sklearn']['davies_bouldin'])\n",
    "            sklearn_metrics['time'].append(self.results[k]['sklearn']['time'])\n",
    "            sklearn_metrics['iterations'].append(self.results[k]['sklearn']['iterations'])\n",
    "\n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        fig.suptitle('K-Means Comparison: Custom vs Sklearn', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 1. Elbow Plot (Inertia)\n",
    "        axes[0, 0].plot(k_values, custom_metrics['inertia'], 'o-', label='Custom', linewidth=2, markersize=8)\n",
    "        axes[0, 0].plot(k_values, sklearn_metrics['inertia'], 's-', label='Sklearn', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_xlabel('Number of Clusters (K)', fontsize=10)\n",
    "        axes[0, 0].set_ylabel('Inertia (Within-cluster SS)', fontsize=10)\n",
    "        axes[0, 0].set_title('Elbow Method', fontweight='bold')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "        # 2. Silhouette Score\n",
    "        axes[0, 1].plot(k_values, custom_metrics['silhouette'], 'o-', label='Custom', linewidth=2, markersize=8)\n",
    "        axes[0, 1].plot(k_values, sklearn_metrics['silhouette'], 's-', label='Sklearn', linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_xlabel('Number of Clusters (K)', fontsize=10)\n",
    "        axes[0, 1].set_ylabel('Silhouette Score', fontsize=10)\n",
    "        axes[0, 1].set_title('Silhouette Score (Higher is Better)', fontweight='bold')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "        # 3. Davies-Bouldin Index\n",
    "        axes[0, 2].plot(k_values, custom_metrics['davies_bouldin'], 'o-', label='Custom', linewidth=2, markersize=8)\n",
    "        axes[0, 2].plot(k_values, sklearn_metrics['davies_bouldin'], 's-', label='Sklearn', linewidth=2, markersize=8)\n",
    "        axes[0, 2].set_xlabel('Number of Clusters (K)', fontsize=10)\n",
    "        axes[0, 2].set_ylabel('Davies-Bouldin Index', fontsize=10)\n",
    "        axes[0, 2].set_title('Davies-Bouldin Index (Lower is Better)', fontweight='bold')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "        # 4. Execution Time\n",
    "        axes[1, 0].bar(np.array(k_values) - 0.2, custom_metrics['time'], width=0.4, label='Custom', alpha=0.8)\n",
    "        axes[1, 0].bar(np.array(k_values) + 0.2, sklearn_metrics['time'], width=0.4, label='Sklearn', alpha=0.8)\n",
    "        axes[1, 0].set_xlabel('Number of Clusters (K)', fontsize=10)\n",
    "        axes[1, 0].set_ylabel('Time (seconds)', fontsize=10)\n",
    "        axes[1, 0].set_title('Execution Time', fontweight='bold')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "        # 5. Iterations to Converge\n",
    "        axes[1, 1].bar(np.array(k_values) - 0.2, custom_metrics['iterations'], width=0.4, label='Custom', alpha=0.8)\n",
    "        axes[1, 1].bar(np.array(k_values) + 0.2, sklearn_metrics['iterations'], width=0.4, label='Sklearn', alpha=0.8)\n",
    "        axes[1, 1].set_xlabel('Number of Clusters (K)', fontsize=10)\n",
    "        axes[1, 1].set_ylabel('Iterations', fontsize=10)\n",
    "        axes[1, 1].set_title('Iterations to Convergence', fontweight='bold')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "        # 6. Cluster Visualization (PCA for 2D)\n",
    "        if self.X.shape[1] > 2:\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(self.X)\n",
    "        else:\n",
    "            X_pca = self.X[:, :2]\n",
    "\n",
    "        # Use optimal k (highest silhouette score)\n",
    "        optimal_k = max(self.results.keys(),\n",
    "                       key=lambda k: self.results[k]['custom']['silhouette'])\n",
    "\n",
    "        custom_labels = self.results[optimal_k]['custom']['labels']\n",
    "\n",
    "        scatter = axes[1, 2].scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                                     c=custom_labels, cmap='viridis',\n",
    "                                     alpha=0.6, s=30)\n",
    "        axes[1, 2].set_xlabel('First Principal Component', fontsize=10)\n",
    "        axes[1, 2].set_ylabel('Second Principal Component', fontsize=10)\n",
    "        axes[1, 2].set_title(f'Cluster Visualization (K={optimal_k})', fontweight='bold')\n",
    "        plt.colorbar(scatter, ax=axes[1, 2], label='Cluster')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('kmeans_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"✓ Saved: kmeans_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save comparison table\n",
    "        comparison_data = []\n",
    "        for k in k_values:\n",
    "            comparison_data.append({\n",
    "                'K': k,\n",
    "                'Method': 'Custom',\n",
    "                'Inertia': self.results[k]['custom']['inertia'],\n",
    "                'Silhouette': self.results[k]['custom']['silhouette'],\n",
    "                'Davies-Bouldin': self.results[k]['custom']['davies_bouldin'],\n",
    "                'Time (s)': self.results[k]['custom']['time'],\n",
    "                'Iterations': self.results[k]['custom']['iterations']\n",
    "            })\n",
    "            comparison_data.append({\n",
    "                'K': k,\n",
    "                'Method': 'Sklearn',\n",
    "                'Inertia': self.results[k]['sklearn']['inertia'],\n",
    "                'Silhouette': self.results[k]['sklearn']['silhouette'],\n",
    "                'Davies-Bouldin': self.results[k]['sklearn']['davies_bouldin'],\n",
    "                'Time (s)': self.results[k]['sklearn']['time'],\n",
    "                'Iterations': self.results[k]['sklearn']['iterations']\n",
    "            })\n",
    "\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df.to_csv('kmeans_comparison_results.csv', index=False)\n",
    "        print(\"✓ Saved: kmeans_comparison_results.csv\")\n",
    "\n",
    "        print(\"\\n✓ All visualizations generated!\")\n",
    "\n",
    "        return comparison_df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# ============================================\n",
    "def compare_kmeans_implementations(X, true_labels=None, k_values=[2, 3, 4, 5, 6],\n",
    "                                   max_samples=10000, random_state=42):\n",
    "    \"\"\"\n",
    "    Main function to compare custom and sklearn K-Means\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like or DataFrame\n",
    "        Data to cluster (use preprocessed features, exclude lat/lon/class)\n",
    "    true_labels : array-like, optional\n",
    "        True class labels if available\n",
    "    k_values : list\n",
    "        List of k values to test\n",
    "    max_samples : int\n",
    "        Maximum number of samples to use (for memory efficiency)\n",
    "    random_state : int\n",
    "        Random seed for sampling\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    comparator : KMeansComparison object\n",
    "    results_df : DataFrame with comparison results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"K-MEANS CLUSTERING: IMPLEMENTATION COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Convert to DataFrame if needed\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    # Select only numeric columns\n",
    "    print(\"\\n[Data Preparation]\")\n",
    "    print(\"-\" * 70)\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X_numeric = X[numeric_cols].copy()\n",
    "\n",
    "    print(f\"Original features: {X.shape[1]}\")\n",
    "    print(f\"Numeric features: {X_numeric.shape[1]}\")\n",
    "    print(f\"Non-numeric features removed: {X.shape[1] - X_numeric.shape[1]}\")\n",
    "    print(f\"Original samples: {X_numeric.shape[0]}\")\n",
    "\n",
    "    # Sample data if too large\n",
    "    if X_numeric.shape[0] > max_samples:\n",
    "        print(f\"\\n⚠ Dataset too large for memory! Sampling {max_samples} rows...\")\n",
    "        np.random.seed(random_state)\n",
    "        sample_indices = np.random.choice(X_numeric.shape[0], max_samples, replace=False)\n",
    "        X_numeric = X_numeric.iloc[sample_indices].reset_index(drop=True)\n",
    "        if true_labels is not None:\n",
    "            if isinstance(true_labels, pd.Series):\n",
    "                true_labels = true_labels.iloc[sample_indices].reset_index(drop=True)\n",
    "            else:\n",
    "                true_labels = true_labels[sample_indices]\n",
    "        print(f\"✓ Sampled to {X_numeric.shape[0]} rows\")\n",
    "\n",
    "    # Remove columns with any missing values\n",
    "    cols_before = X_numeric.shape[1]\n",
    "    X_numeric = X_numeric.dropna(axis=1)\n",
    "    if X_numeric.shape[1] < cols_before:\n",
    "        print(f\"Removed {cols_before - X_numeric.shape[1]} columns with NaN values\")\n",
    "\n",
    "    # Check for any remaining issues\n",
    "    if X_numeric.isnull().sum().sum() > 0:\n",
    "        print(\"Warning: Found NaN values, filling with column means...\")\n",
    "        X_numeric = X_numeric.fillna(X_numeric.mean())\n",
    "\n",
    "    # Check for infinite values\n",
    "    if np.isinf(X_numeric.values).sum() > 0:\n",
    "        print(\"Warning: Found infinite values, replacing...\")\n",
    "        X_numeric = X_numeric.replace([np.inf, -np.inf], np.nan)\n",
    "        X_numeric = X_numeric.fillna(X_numeric.mean())\n",
    "\n",
    "    # Further reduce features if still too many\n",
    "    if X_numeric.shape[1] > 50:\n",
    "        print(f\"\\n⚠ Too many features ({X_numeric.shape[1]})! Selecting top 30 by variance...\")\n",
    "        variances = X_numeric.var()\n",
    "        top_features = variances.nlargest(30).index\n",
    "        X_numeric = X_numeric[top_features]\n",
    "        print(f\"✓ Reduced to {X_numeric.shape[1]} features\")\n",
    "\n",
    "    # Convert to float32 to save memory\n",
    "    X_numeric = X_numeric.astype(np.float32)\n",
    "\n",
    "    print(f\"\\n✓ Final dataset shape: {X_numeric.shape}\")\n",
    "    print(f\"✓ Memory usage: {X_numeric.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"✓ Ready for clustering!\")\n",
    "\n",
    "    # Initialize comparator\n",
    "    comparator = KMeansComparison(X_numeric.values, true_labels)\n",
    "\n",
    "    # Run comparison\n",
    "    comparator.run_comparison(k_values=k_values, random_state=random_state)\n",
    "\n",
    "    # Generate visualizations\n",
    "    results_df = comparator.plot_comparison()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nComparison Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # Determine optimal k\n",
    "    best_k = max(comparator.results.keys(),\n",
    "                 key=lambda k: comparator.results[k]['custom']['silhouette'])\n",
    "    print(f\"\\n✓ Optimal K (based on Silhouette Score): {best_k}\")\n",
    "\n",
    "    return comparator, results_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n1. Load preprocessed data:\")\n",
    "    train_data = pd.read_csv('train_data_preprocessed.csv')\n",
    "    print(\"\\n2. Prepare features (automatically selects numeric columns):\")\n",
    "    X = train_data.drop('class', axis=1, errors='ignore')\n",
    "    y = train_data['class'] if 'class' in train_data.columns else None\n",
    "    print(\"\\n3. Run comparison (with memory-efficient sampling):\")\n",
    "    comparator, results = compare_kmeans_implementations(\n",
    "        X,\n",
    "        true_labels=y,\n",
    "        k_values=[2, 3, 4, 5],\n",
    "        max_samples=500_000  # Adjust based on your RAM\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
