{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca55421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reduce_memory_usage(df):\n",
    "    \"\"\"Reduce memory usage by downcasting numeric types\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object and col not in ['latitude', 'longitude']:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization: {end_mem:.2f} MB')\n",
    "    print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    return df\n",
    "\n",
    "def preprocess_wildfire_data_efficient(csv_path, chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Memory-efficient preprocessing for large wildfire datasets\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"MEMORY-EFFICIENT WILDFIRE PREPROCESSING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ============================================\n",
    "    # 1. LOAD DATA IN CHUNKS AND GET INFO\n",
    "    # ============================================\n",
    "    print(\"\\n[1/7] Loading data information...\")\n",
    "\n",
    "    # Get basic info and column types\n",
    "    sample = pd.read_csv(csv_path, nrows=1000)\n",
    "    print(f\"Sample loaded. Columns: {len(sample.columns)}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 2. PROCESS IN CHUNKS\n",
    "    # ============================================\n",
    "    print(\"\\n[2/7] Processing data in chunks to save memory...\")\n",
    "\n",
    "    chunks_processed = []\n",
    "    chunk_num = 0\n",
    "\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n",
    "        chunk_num += 1\n",
    "        print(f\"\\nProcessing chunk {chunk_num} ({len(chunk)} rows)...\")\n",
    "\n",
    "        # Convert ORG_CARBON to numeric\n",
    "        if 'ORG_CARBON' in chunk.columns:\n",
    "            chunk['ORG_CARBON'] = pd.to_numeric(chunk['ORG_CARBON'], errors='coerce')\n",
    "\n",
    "        # Fill missing values\n",
    "        numeric_cols = chunk.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if chunk[col].isnull().sum() > 0:\n",
    "                chunk[col].fillna(chunk[col].median(), inplace=True)\n",
    "\n",
    "        # Feature Engineering (simplified to save memory)\n",
    "        if 'tmax' in chunk.columns and 'tmin' in chunk.columns:\n",
    "            chunk['temp_range'] = chunk['tmax'] - chunk['tmin']\n",
    "            chunk['temp_mean'] = (chunk['tmax'] + chunk['tmin']) / 2\n",
    "\n",
    "        if 'tmax' in chunk.columns and 'precipitation' in chunk.columns:\n",
    "            chunk['aridity_index'] = np.where(\n",
    "                chunk['precipitation'] > 0.1,\n",
    "                chunk['tmax'] / chunk['precipitation'],\n",
    "                chunk['tmax'] * 10\n",
    "            )\n",
    "\n",
    "        if 'precipitation' in chunk.columns and 'temp_mean' in chunk.columns:\n",
    "            chunk['drought_stress'] = np.where(\n",
    "                chunk['precipitation'] > 0.1,\n",
    "                chunk['temp_mean'] / chunk['precipitation'],\n",
    "                chunk['temp_mean'] * 10\n",
    "            )\n",
    "\n",
    "        if all(col in chunk.columns for col in ['SAND', 'CLAY']):\n",
    "            chunk['sand_clay_ratio'] = np.where(\n",
    "                chunk['CLAY'] > 1,\n",
    "                chunk['SAND'] / chunk['CLAY'],\n",
    "                chunk['SAND']\n",
    "            )\n",
    "\n",
    "        if all(col in chunk.columns for col in ['tmax', 'precipitation', 'ORG_CARBON']):\n",
    "            chunk['fire_risk_score'] = np.where(\n",
    "                chunk['precipitation'] > 0.1,\n",
    "                (chunk['tmax'] * chunk['ORG_CARBON']) / chunk['precipitation'],\n",
    "                chunk['tmax'] * chunk['ORG_CARBON'] * 10\n",
    "            )\n",
    "\n",
    "        # Cap extreme values\n",
    "        for col in chunk.select_dtypes(include=[np.number]).columns:\n",
    "            if col not in ['latitude', 'longitude', 'class']:\n",
    "                chunk[col] = chunk[col].replace([np.inf, -np.inf], np.nan)\n",
    "                if chunk[col].isnull().sum() > 0:\n",
    "                    chunk[col].fillna(chunk[col].median(), inplace=True)\n",
    "                # Cap at 99.9th percentile to remove extreme outliers\n",
    "                upper_limit = chunk[col].quantile(0.999)\n",
    "                lower_limit = chunk[col].quantile(0.001)\n",
    "                chunk[col] = chunk[col].clip(lower_limit, upper_limit)\n",
    "\n",
    "        # Reduce memory\n",
    "        chunk = reduce_memory_usage(chunk)\n",
    "\n",
    "        chunks_processed.append(chunk)\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "    # ============================================\n",
    "    # 3. COMBINE CHUNKS\n",
    "    # ============================================\n",
    "    print(\"\\n[3/7] Combining processed chunks...\")\n",
    "    data = pd.concat(chunks_processed, ignore_index=True)\n",
    "    del chunks_processed\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Total rows: {len(data)}\")\n",
    "    print(f\"Total columns: {len(data.columns)}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 4. HANDLE CATEGORICAL VARIABLES (SIMPLIFIED)\n",
    "    # ============================================\n",
    "    print(\"\\n[4/7] Encoding categorical variables...\")\n",
    "\n",
    "    # Only keep most important categorical features\n",
    "    if 'LCCCODE' in data.columns:\n",
    "        # Keep only top 5 categories\n",
    "        top_cats = data['LCCCODE'].value_counts().nlargest(5).index\n",
    "        data['LCCCODE'] = data['LCCCODE'].apply(lambda x: x if x in top_cats else 'Other')\n",
    "        dummies = pd.get_dummies(data['LCCCODE'], prefix='LCC', drop_first=True, dtype=np.int8)\n",
    "        data = pd.concat([data, dummies], axis=1)\n",
    "        data.drop('LCCCODE', axis=1, inplace=True)\n",
    "\n",
    "    # Drop other high-cardinality categorical columns to save memory\n",
    "    cats_to_drop = ['TEXTURE_USDA', 'TEXTURE_SOTER', 'SMU', 'WISE30s_SMU_ID']\n",
    "    for col in cats_to_drop:\n",
    "        if col in data.columns:\n",
    "            data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # ============================================\n",
    "    # 5. PREPARE FOR SCALING\n",
    "    # ============================================\n",
    "    print(\"\\n[5/7] Preparing features for scaling...\")\n",
    "\n",
    "    protected_features = ['latitude', 'longitude', 'class']\n",
    "    features_to_scale = [col for col in data.columns\n",
    "                         if col not in protected_features\n",
    "                         and data[col].dtype in [np.float32, np.float64, np.int8, np.int16, np.int32, np.int64]]\n",
    "\n",
    "    print(f\"Features to scale: {len(features_to_scale)}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 6. SPLIT FIRST, THEN SCALE\n",
    "    # ============================================\n",
    "    print(\"\\n[6/7] Splitting data...\")\n",
    "\n",
    "    X = data.drop(['class'], axis=1)\n",
    "    y = data['class']\n",
    "\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 7. SCALE ONLY TRAINING DATA\n",
    "    # ============================================\n",
    "    print(\"\\n[7/7] Scaling features (Standard Scaler only)...\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "    X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "    print(\"\\n✓ Preprocessing complete!\")\n",
    "\n",
    "    # ============================================\n",
    "    # 8. SAVE TO CSV\n",
    "    # ============================================\n",
    "    print(\"\\n[8/8] Saving to CSV files...\")\n",
    "\n",
    "    train_data = X_train.copy()\n",
    "    train_data['class'] = y_train.values\n",
    "    test_data = X_test.copy()\n",
    "    test_data['class'] = y_test.values\n",
    "\n",
    "    train_data.to_csv('train_data_preprocessed.csv', index=False)\n",
    "    print(\"✓ Saved: train_data_preprocessed.csv\")\n",
    "\n",
    "    test_data.to_csv('test_data_preprocessed.csv', index=False)\n",
    "    print(\"✓ Saved: test_data_preprocessed.csv\")\n",
    "\n",
    "    # Save feature names\n",
    "    feature_info = pd.DataFrame({\n",
    "        'feature_name': X_train.columns.tolist()\n",
    "    })\n",
    "    feature_info.to_csv('feature_names.csv', index=False)\n",
    "    print(\"✓ Saved: feature_names.csv\")\n",
    "\n",
    "    # ============================================\n",
    "    # SUMMARY\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "    print(f\"Total features: {len(X_train.columns)}\")\n",
    "    print(f\"\\nClass distribution in training set:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(f\"\\nClass balance: {y_train.value_counts(normalize=True).round(3)}\")\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': X_train.columns.tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    results = preprocess_wildfire_data_efficient('/content/merged_data_with_soil.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
